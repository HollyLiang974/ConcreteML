{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 使用整数训练前馈神经网络\n",
    "Iris是一个非常著名的数据集，它包含了150个样本，分为3类，每类50个样本，每个样本包含4个属性，分别是花萼长度、花萼宽度、花瓣长度、花瓣宽度，目标是根据这4个属性预测鸢尾花的类别。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 导入依赖库"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import numpy\n",
    "from concrete import fhe\n",
    "from concrete.ml.quantization import QuantizedArray\n",
    "from concrete.ml.quantization.quantized_ops import (\n",
    "    QuantizedGemm,\n",
    "    QuantizedSigmoid,\n",
    "    QuantizedSub,\n",
    "    QuantizedMul,\n",
    "    QuantizedDiv,\n",
    ")\n",
    "from concrete.ml.quantization.quantizers import(\n",
    "    QuantizedArray,\n",
    "    MinMaxQuantizationStats,\n",
    "    QuantizationOptions,\n",
    "    UniformQuantizationParameters\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T22:23:43.019701Z",
     "end_time": "2023-10-06T22:23:43.042795Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 加载数据集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# 加载Iris数据集\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 将输出标签进行独热编码\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_one_hot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T22:23:43.031731Z",
     "end_time": "2023-10-06T22:23:43.077484Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 定义NumPy版的FFNN模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# 定义一个类来表示NumPy版的FFNN模型\n",
    "class NumPyFFNN:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weights1 = np.random.randn(input_dim, hidden_dim)\n",
    "        self.bias1 = np.zeros(hidden_dim)\n",
    "        self.weights2 = np.random.randn(hidden_dim, output_dim)\n",
    "        self.bias2 = np.zeros(output_dim)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def q_sub(self, n_bits:int,input_0 :numpy.ndarray, input_1:numpy.ndarray):\n",
    "        # Quantize the inputs with n_bits\n",
    "        q_inputs_0 = QuantizedArray(n_bits, input_0, is_signed=True)\n",
    "        q_inputs_1 = QuantizedArray(n_bits, input_1, is_signed=True)\n",
    "\n",
    "        q_op = QuantizedSub(n_bits, QuantizedSub.__name__, int_input_names={\"0\", \"1\"})\n",
    "        raw_output_vv = q_op.calibrate(input_0, input_1)\n",
    "        # print(\"raw_output_vv:\\n\", raw_output_vv)\n",
    "        quantized_output_vv = q_op(q_inputs_0, q_inputs_1).dequant()\n",
    "        # print(\"quantized_output_vv:\\n\", quantized_output_vv)\n",
    "\n",
    "        return  quantized_output_vv\n",
    "\n",
    "    def q_mul(self, n_bits:int,input_0 :numpy.ndarray, input_1:numpy.ndarray):\n",
    "        # Quantize the inputs with n_bits\n",
    "        q_inputs_0 = QuantizedArray(n_bits, input_0, is_signed=True)\n",
    "        q_inputs_1 = QuantizedArray(n_bits, input_1, is_signed=True)\n",
    "\n",
    "        q_op = QuantizedMul(\n",
    "            n_bits, QuantizedMul.__name__, int_input_names={\"0\"}, constant_inputs={\"b\": q_inputs_1}\n",
    "        )\n",
    "        raw_output_vv = q_op.calibrate(input_0)\n",
    "        # print(\"raw_output_vv:\\n\", raw_output_vv)\n",
    "        quantized_output_vv = q_op(q_inputs_0).dequant()\n",
    "        # print(\"quantized_output_vv:\\n\", quantized_output_vv)\n",
    "        return quantized_output_vv\n",
    "\n",
    "    def q_div(self, n_bits:int,input_0 :numpy.ndarray, input_1:numpy.ndarray):\n",
    "        # Quantize the inputs with n_bits\n",
    "        q_inputs_0 = QuantizedArray(n_bits, input_0, is_signed=True)\n",
    "        q_inputs_1 = QuantizedArray(n_bits, input_1, is_signed=True)\n",
    "\n",
    "        q_op = QuantizedDiv(\n",
    "            n_bits, QuantizedDiv.__name__, int_input_names={\"0\"}, constant_inputs={\"b\": q_inputs_1}\n",
    "        )\n",
    "        raw_output_vv = q_op.calibrate(input_0)\n",
    "        # print(\"raw_output_vv:\\n\", raw_output_vv)\n",
    "        quantized_output_vv = q_op(q_inputs_0).dequant()\n",
    "        # print(\"quantized_output_vv:\\n\", quantized_output_vv)\n",
    "        return quantized_output_vv\n",
    "\n",
    "    def q_gemm_(\n",
    "        self,\n",
    "        n_bits: int,\n",
    "        inputs: numpy.ndarray,\n",
    "        weights: numpy.ndarray,\n",
    "        bias: numpy.ndarray,\n",
    "    ):\n",
    "        OP_DEBUG_NAME = \"Test_\"\n",
    "        q_inputs = QuantizedArray(n_bits, inputs)\n",
    "        q_weights = QuantizedArray(n_bits, weights, is_signed=True)\n",
    "        q_gemm = QuantizedGemm(\n",
    "            n_bits,\n",
    "            OP_DEBUG_NAME + \"QuantizedGemm\",\n",
    "            int_input_names={\"0\"},\n",
    "            constant_inputs={\"b\": q_weights, \"c\": bias},\n",
    "\n",
    "        )\n",
    "\n",
    "        # Calibrate the Quantized layer\n",
    "        q_gemm.produces_graph_output = True\n",
    "        q_gemm.calibrate(inputs)\n",
    "        actual_gemm_output = q_gemm(q_inputs).dequant()\n",
    "        # print(\"actual_gemm_output:\\n\", actual_gemm_output)\n",
    "\n",
    "        return actual_gemm_output\n",
    "\n",
    "    def q_gemm_no_b(\n",
    "        self,\n",
    "        n_bits: int,\n",
    "        inputs: numpy.ndarray,\n",
    "        weights: numpy.ndarray,\n",
    "    ):\n",
    "        OP_DEBUG_NAME = \"Test_\"\n",
    "        q_inputs = QuantizedArray(n_bits, inputs)\n",
    "        q_weights = QuantizedArray(n_bits, weights, is_signed=True)\n",
    "        q_gemm = QuantizedGemm(\n",
    "            n_bits,\n",
    "            OP_DEBUG_NAME + \"QuantizedGemm\",\n",
    "            int_input_names={\"0\"},\n",
    "            constant_inputs={\"b\": q_weights},\n",
    "\n",
    "        )\n",
    "\n",
    "        # Calibrate the Quantized layer\n",
    "        q_gemm.produces_graph_output = True\n",
    "        q_gemm.calibrate(inputs)\n",
    "        actual_gemm_output = q_gemm(q_inputs).dequant()\n",
    "        # print(\"actual_gemm_output:\\n\", actual_gemm_output)\n",
    "\n",
    "        return actual_gemm_output\n",
    "\n",
    "    def q_sigmoid(self,n_bits: int, inputs:numpy.ndarray):\n",
    "        q_inputs = QuantizedArray(n_bits, inputs,is_signed=True)\n",
    "        quantized_op = QuantizedSigmoid(n_bits, QuantizedSigmoid)\n",
    "        expected_output = quantized_op.calibrate(inputs)\n",
    "        # print(\"expected_output:\\n\", expected_output)\n",
    "        q_output = quantized_op(q_inputs)\n",
    "        values = q_output.values\n",
    "        # print(\"values:\\n\", values)\n",
    "        return values\n",
    "\n",
    "    def q_np_sum(self,n_bits:int, inputs:numpy.ndarray):\n",
    "        options=QuantizationOptions(n_bits,is_symmetric=True,is_signed=True)\n",
    "        stats=MinMaxQuantizationStats(n_bits)\n",
    "        stats.compute_quantization_stats(np.sum(inputs, axis=0))\n",
    "        q_inputs= QuantizedArray(n_bits,inputs,is_signed=True,stats=stats,options=options)\n",
    "        q_result=np.sum(q_inputs.qvalues,axis=0)\n",
    "        deq_inputs=q_result*q_inputs.quantizer.scale\n",
    "        return deq_inputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = np.dot(x, self.weights1) + self.bias1\n",
    "        out1_sigmoid = self.sigmoid(out1)  # 使用Sigmoid激活函数\n",
    "        out2 = np.dot(out1_sigmoid, self.weights2) + self.bias2\n",
    "        return out2\n",
    "\n",
    "    def q_forward(self, x):\n",
    "        n_bits=4\n",
    "        # 线性层\n",
    "        out1 = self.q_gemm_(n_bits,x,self.weights1,self.bias1)\n",
    "        # 激活层\n",
    "        out1_sigmoid = self.q_sigmoid(n_bits,out1)  # 使用Sigmoid激活函数\n",
    "        out2=self.q_gemm_(n_bits,out1_sigmoid, self.weights2, self.bias2)\n",
    "        return out2\n",
    "\n",
    "    def backward(self, x, y, learning_rate):\n",
    "        # 前向传播\n",
    "        out1 = np.dot(x, self.weights1) + self.bias1\n",
    "        out1_sigmoid = self.sigmoid(out1)  # 使用Sigmoid激活函数\n",
    "        out2 = np.dot(out1_sigmoid, self.weights2) + self.bias2\n",
    "        loss = np.mean((out2 - y) ** 2)  # 均方误差损失\n",
    "\n",
    "        # 反向传播\n",
    "        delta_out2 = 2 * (out2 - y) / len(x)\n",
    "        delta_weights2 = np.dot(out1_sigmoid.T, delta_out2)\n",
    "        delta_bias2 = np.sum(delta_out2, axis=0)\n",
    "        delta_out1 = np.dot(delta_out2, self.weights2.T)\n",
    "        delta_out1_sigmoid = delta_out1 * out1_sigmoid * (1 - out1_sigmoid)  # Sigmoid的导数\n",
    "        delta_weights1 = np.dot(x.T, delta_out1_sigmoid)\n",
    "        delta_bias1 = np.sum(delta_out1_sigmoid, axis=0)\n",
    "\n",
    "        # 更新参数\n",
    "        self.weights2 -= learning_rate * delta_weights2\n",
    "        self.bias2 -= learning_rate * delta_bias2\n",
    "        self.weights1 -= learning_rate * delta_weights1\n",
    "        self.bias1 -= learning_rate * delta_bias1\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def q_backward(self, x, y, learning_rate):\n",
    "        n_bits = 8\n",
    "        # 前向传播\n",
    "        out1 = self.q_gemm_(n_bits, x, self.weights1, self.bias1)\n",
    "        out1_sigmoid = self.q_sigmoid(n_bits, out1)  # 使用Sigmoid激活函数\n",
    "        out2 = self.q_gemm_(n_bits, out1_sigmoid, self.weights2, self.bias2)\n",
    "        loss = np.mean((out2 - y) ** 2)  # 均方误差损失\n",
    "\n",
    "        # 反向传播\n",
    "        # delta_out2 = 2 * (out2 - y) / len(x)  #out2是密文，y是密文，所以delta_out2是密文\n",
    "        delta_out2=self.q_div(n_bits,self.q_mul(n_bits,self.q_sub(n_bits,out2,y),2),len(x))\n",
    "        # delta_weights2 = np.dot(out1_sigmoid.T, delta_out2) #out1_sigmoid.T是密文，delta_out2是密文，所以delta_weights2是密文，\n",
    "        delta_weights2=self.q_gemm_no_b(n_bits,out1_sigmoid.T,delta_out2)                                             # 密文和密文矩阵乘法不能超过16位\n",
    "        # delta_bias2 = np.sum(delta_out2, axis=0)\n",
    "        delta_bias2 = self.q_np_sum(8,delta_out2) #delta_out2是密文，所以delta_bias2是密文\n",
    "\n",
    "        # delta_out1 = np.dot(delta_out2, self.weights2.T)\n",
    "        delta_out1=self.q_gemm_no_b(n_bits,delta_out2,self.weights2.T)\n",
    "        # delta_out1_sigmoid = delta_out1 * out1_sigmoid * (1 - out1_sigmoid)  # Sigmoid的导数\n",
    "        delta_out1_sigmoid=self.q_mul(n_bits,self.q_mul(n_bits,delta_out1,out1_sigmoid),self.q_sub(n_bits,1,out1_sigmoid))\n",
    "        # delta_weights1 = np.dot(x.T, delta_out1_sigmoid)\n",
    "        delta_weights1=self.q_gemm_no_b(n_bits,x.T,delta_out1_sigmoid)\n",
    "        # delta_bias1 = np.sum(delta_out1_sigmoid, axis=0)\n",
    "        delta_bias1 = self.q_np_sum(8,delta_out1_sigmoid)\n",
    "\n",
    "        # 更新参数\n",
    "        # self.weights2 -= learning_rate * delta_weights2\n",
    "        self.weights2 -= self.q_mul(n_bits,learning_rate,delta_weights2)\n",
    "        # self.bias2 -= learning_rate * delta_bias2\n",
    "        self.bias2 -= self.q_mul(n_bits,learning_rate,delta_bias2)\n",
    "        # self.weights1 -= learning_rate * delta_weights1\n",
    "        self.weights1 -= self.q_mul(n_bits,learning_rate,delta_weights1)\n",
    "        # self.bias1 -= learning_rate * delta_bias1\n",
    "        self.bias1 -= self.q_mul(n_bits,learning_rate,delta_bias1)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "# 模型参数\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = y_train.shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T22:23:43.083496Z",
     "end_time": "2023-10-06T22:23:43.090542Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 创建NumPy模型实例"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "numpy_model = NumPyFFNN(input_dim, hidden_dim, output_dim)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T22:23:43.083496Z",
     "end_time": "2023-10-06T22:23:43.090542Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 训练模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.16602038528811128\n",
      "Epoch [200/1000], Loss: 0.15869512200901284\n",
      "Epoch [300/1000], Loss: 0.1439700651322571\n",
      "Epoch [400/1000], Loss: 0.09976505536737819\n",
      "Epoch [500/1000], Loss: 0.09538336104414728\n",
      "Epoch [600/1000], Loss: 0.10424544527365569\n",
      "Epoch [700/1000], Loss: 0.045467719131744716\n",
      "Epoch [800/1000], Loss: 0.06518831870663637\n",
      "Epoch [900/1000], Loss: 0.09468270588078664\n",
      "Epoch [1000/1000], Loss: 0.055506230078776496\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 随机选择一批训练数据\n",
    "    batch_indices = np.random.choice(len(X_train), 32, replace=False)\n",
    "    x_batch = X_train[batch_indices]\n",
    "    y_batch = y_train[batch_indices]\n",
    "\n",
    "    # 执行一次前向传播和反向传播，并获得损失\n",
    "    loss = numpy_model.q_backward(x_batch, y_batch, learning_rate)\n",
    "\n",
    "    # 打印损失\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T22:23:43.083496Z",
     "end_time": "2023-10-06T22:24:48.833530Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 测试模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 93.33%\n"
     ]
    }
   ],
   "source": [
    "# 前向传播\n",
    "predictions = numpy_model.forward(X_test)\n",
    "\n",
    "# 计算准确率\n",
    "correct = (np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)).sum()\n",
    "total = len(X_test)\n",
    "accuracy = correct / total * 100\n",
    "\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-06T22:24:48.832507Z",
     "end_time": "2023-10-06T22:24:48.833530Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
